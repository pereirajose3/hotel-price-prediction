library(VIM) #Para visualização e imputação de valores omissos (KNN imputation)
library(ggplot2) #Para visualização de dados
library(purrr) # Para usar vetores
library(VIM) #Para visualização e imputação de valores omissos (KNN imputation)
library(ggplot2) #Para visualização de dados
library(purrr) # Para usar vetores
library(tidyr) # Para a estrutura de dados
library(psych) # Para gráficos
library(e1071)# para calcular Skewness
library(neuralnet)# Para redes neuronais
library(MASS)# Para a regressão linear múltipla
library(reshape2) # Para transformar dados
library(car) # Para regressão
library(caret)# Para validaçao cruzada
library(Metrics)# Para medidas de performance de modelos
library(partykit)# Para realizar árvores de decisão
library(rpart)# Para realizar árvores de decisão
library(rpart.plot) #Para realizar árvores de decisão
library(caret)# Para modelação
library(dplyr)# Para manipulação de dados
library(plyr) # Para mapvalues() function
library(corrplot)# Para fazer gráficos de correlações
library(cowplot)# Para fazer grid plot
library (gbm)# Para fazer o gradiente Descendente
df<-read.csv('e:/df_Pequim.csv',header = TRUE, sep=',',dec = '.')
df<-read.csv('df_Pequim.csv',header = TRUE, sep=',',dec = '.')
data<-df
#Dimensão do dataset
dim(data)
#Nome das variáveis
names(data)
#Visualizar as primeiras 6 linhas
head(data)
#Remover a variável nome de Hotel
data<-data[,-1]
#Visualizar o tipo das variáveis
str(data)
#Visualizar valores descritivos das variáveis
summary(data)
#Visualizar novamente valores descritivos das variáveis
summary(data)
#converter as variáveis lógicas em 0 e 1, uma vez que algoritmos de aprendizagem automática lidam melhore com valores numéricos
data$cancel<-data$cancel*1
data$rooms<-data$rooms*1
data$breakfast<-data$breakfast*1
#Histograma de todas as variáveis numéricas
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Tratar valores omissos da variável score review através do método da imputação vizinhos mais próximos
set.seed(123)
summary(data)
imputdata1 <- data
imputdata1 <- kNN(data, variable = "score_review", k = 17)
summary(imputdata1)
#K é escolhido através da raiz quadrada do número de observações totais do dataset
summary(imputdata1)
ncol(imputdata1)
head(imputdata1)
#Uma nova coluna de valores lógicos foi adicionada no final do dataset, deve proceder-se à sua remoção
imputdata1 <- subset(imputdata1,select = price:stars)
head(imputdata1)
summary(imputdata1)
data <- imputdata1
#Confirmar a não existência de NA's
apply(data,2,function(x) sum(is.na(x)))
#Visualizar novamente o Histograma de todas as variáveis numéricas após o tratamento dos dados omissos
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Visualização da variável estrelas com ggplot -geometric density
ggplot(data, aes(stars)) + geom_density(adjust = 1/5)
#A análise de densidade da variável sugere a transformação da variável em categórica ordinal
#Alguns alojamentos não possuem classificação por estrelas, de forma a resolver este problema procedeu-se à criação
#de contentores (bins) [0, 1-3, 4-5] de forma a converter a variável estrelas em categórica ordinal
#Criar categorias :
data$stars[data$stars==0]<-'0'
data$stars[data$stars>=1 & data$stars<=3]<-'1-3'
data$stars[data$stars>=4 & data$stars<=5]<-'4-5'
#De forma a melhorar a performance dos algoritmos de aprendizagem automática, de seguida procedeu-se à realização de ordinal encoding
#para a variável estrelas.
#Função para realizar ordinal encoding
encode_ordinal <- function(x, order = unique(x)) {
x <- as.numeric(factor(x, levels = order, exclude = NULL))
x
}
#Aplicar ordinal encoding à variável estrelas
data$stars<- encode_ordinal(data$stars, order = c('0', '1-3', '4-5'))
#Visualizar correlações entre variáveis numéricas através do valor de correlações e do scatter plot
pairs.panels(data[c(1,2,6,7)],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = FALSE,  # show density plots
ellipses = FALSE # show correlation ellipses
)
## Visualização da variável score_review com ggplot -geometric density
ggplot(data, aes(score_review)) + geom_density(adjust = 1/5)
#Verificar os quartis das variáveis numéricas
quantile(data$price)
quantile(data$distance)
quantile(data$ncomments)
#Pela análise dos histogramas parece existir alguns outliers, vamos usar o boxplot para cada variável numérica para confirmar
#which devolve a linha do hotel que constitui um outlier
#Tem que se usar a variável df com os dados originais para identificar os hotéis pertencentes aos outliers
#Verificar quais são os outliers da variável preço
ggplot(data, aes(x="", y=price))+geom_boxplot()
df[which(data$price>quantile(data$price,prob=0.75)+1.5*(quantile(data$price,prob=0.75)-quantile(data$price,prob=0.25))),c(1,2)]
df[which(data$price<quantile(data$price,prob=0.25)-1.5*(quantile(data$price,prob=0.75)-quantile(data$price,prob=0.25))),c(1,2)]
#Verificar quais são os outliers da variável distância
ggplot(data, aes(x="", y=distance))+geom_boxplot()
df[which(data$distance>quantile(data$distance,prob=0.75)+1.5*(quantile(data$distance,prob=0.75)-quantile(data$distance,prob=0.25))),c(1,7)]
df[which(data$distance<quantile(data$distance,prob=0.25)-1.5*(quantile(data$distance,prob=0.75)-quantile(data$distance,prob=0.25))),c(1,7)]
#Verificar quais são os outliers da variável número de comentários
ggplot(data, aes(x="", y=ncomments))+geom_boxplot()
df[which(data$ncomments>quantile(data$ncomments,prob=0.75)+1.5*(quantile(data$ncomments,prob=0.75)-quantile(data$ncomments,prob=0.25))),c(1,3)]
df[which(data$ncomments<quantile(data$ncomments,prob=0.25)-1.5*(quantile(data$ncomments,prob=0.75)-quantile(data$ncomments,prob=0.25))),c(1,3)]
#De seguida procedeu-se à remoção de 1 outlier situado numa posição muito extrema de forma a melhorar a distribuição da variabilidade dos dados
#Remoção da observação com preço igual a 1130
data<-data[-c(which(data$price==1130)), ]
#Através da análise da distribuição e dos outliers foi possível verificar que que algo tem que ser feito para corrigir a distribuição dos dados,
#uma vez que temos uma assimetria dos dados bastante elevada com muitos outliers. Algoritmos de machine learning não lidam bem com distribuições
#assimétricas nem com outliers. Desta forma, vamos de seguida proceder ao logaritmo das observações
#das variaveis distância, número de comentários e preço.
#Verificar novamente as distribuições das variáveis antes de proceder ao logaritmo das variáveis
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#De seguida separa-se um Dataframe sem transformação de logaritmo para caso seja necessário, se possa mais tarde utilizar
data_no_log<-data
#Aplicar os logaritmos às variáveis
data$distance<-log(data$distance)
data$ncomments<-log1p(data$ncomments)# Soma 1 aos dados visto que a variável tem valores nulos
data$price<-log(data$price)
#Verificar a assimetria da distribuição
skewness(data$distance)
skewness(data$ncomments)
skewness(data$price)
#Verificar a distribuição depois de serem aplicados os logaritmos
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Visualizar novamente correlações entre variáveis numéricas através do valor de correlações e scatter plot, após a aplicação do logaritmo
pairs.panels(data[c(1,2,6,7)],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = FALSE,  # show density plots
ellipses = FALSE # show correlation ellipses
)
#Verificar novamente outliers após a aplicação do logaritmo
ggplot(data, aes(x="", y=distance))+geom_boxplot()
ggplot(data, aes(x="", y=ncomments))+geom_boxplot()
ggplot(data, aes(x="", y=price))+geom_boxplot()
#De seguida procede-se à separação entre treino e teste
#75% of the sample size
smp_size <- floor(0.75 * nrow(data))
## Definir a semente para obter sempre a mesma separação
set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
#Normalização dos dados de treino (Média 0 e Desvio Padrão 1)
train_scale<-data_no_log[train_ind, ]
for (i in 1:ncol(train)) train_scale[,i]<-(train[,i]-mean(train[,i]))/sd(train[,i])
#Verificar
round(colMeans(train_scale[,c(1:ncol(train_scale))]),2)
apply(train_scale,2,sd)
#Histograma de dados de treino com estandardização MinMax
train_scale[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Normalização Min-Max
train_MinMax <- data_no_log[train_ind, ]
for (i in 1:ncol(train_MinMax)) train_MinMax[,i]<-(train_MinMax[,i]-min(train_MinMax[,i]))/(max(train_MinMax[,i])-min(train_MinMax[,i]))
#Verificar
apply(train_MinMax,2,max)
apply(train_MinMax,2,min)
#Podemos verificar que todas as variáveis foram estandardizadas
#Histograma de dados de treino com estandardização MinMax
train_MinMax[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Dados de treino sem logaritmo
train_no_log <- data_no_log[train_ind, ]
###########################################################################################################################
#                                            Regressão linear múltipla                                                    #
###########################################################################################################################
# Regressão linear múltipla
lm1 <- lm(train$price ~ ncomments+distance+score_review, data=train)
############################################################################################################################
############################################################################################################################
###############                                   Notas importantes                                      ###################
############################################################################################################################
############################################################################################################################
###############                     O logaritmo foi aplicado ao treino e ao teste                        ###################
############################################################################################################################
############################################################################################################################
############################################################################################################################
###############          Dependendo dos algoritmos de aprendizagem automática poderão ser utilizados       ###################
###############                               os diferentes datsets de treino:                           ###################
############################################################################################################################
############################################################################################################################
############################################################################################################################
#'train_no_log' - Dados de treino sem aplicação de logaritmo às variáveis preço, distância e número de comentário
#‘train’- Dados de treino com aplicação de logaritmo às variaveis preço, distancia e número de comentário
#‘train_scale’ - Dados de treino com normalização de média 0 e desvio padrão 1
#‘train_MinMax’ - Dados de treino com estandardização de mínimo 0 e máximo 1
###########################################################################################################################
#                                            Regressão linear múltipla                                                    #
###########################################################################################################################
# Regressão linear múltipla
lm1 <- lm(train$price ~ ncomments+distance+score_review, data=train)
summary(lm1)
#Análise de resíduos
par(mfrow = c(2, 2))
plot(lm1)
library(VIM) #Para visualização e imputação de valores omissos (KNN imputation)
library(ggplot2) #Para visualização de dados
library(purrr) # Para usar vetores
library(tidyr) # Para a estrutura de dados
library(psych) # Para gráficos
library(e1071)# para calcular Skewness
library(neuralnet)# Para redes neuronais
library(MASS)# Para a regressão linear múltipla
library(reshape2) # Para transformar dados
library(car) # Para regressão
library(caret)# Para validaçao cruzada
library(Metrics)# Para medidas de performance de modelos
library(partykit)# Para realizar árvores de decisão
library(rpart)# Para realizar árvores de decisão
library(rpart.plot) #Para realizar árvores de decisão
library(caret)# Para modelação
library(dplyr)# Para manipulação de dados
library(plyr) # Para mapvalues() function
library(corrplot)# Para fazer gráficos de correlações
library(cowplot)# Para fazer grid plot
library (gbm)# Para fazer o gradiente Descendente
df<-read.csv('df_Pequim.csv',header = TRUE, sep=',',dec = '.')
data<-df
#Dimensão do dataset
dim(data)
#Nome das variáveis
names(data)
#Visualizar as primeiras 6 linhas
head(data)
#Remover a variável nome de Hotel
data<-data[,-1]
#Visualizar o tipo das variáveis
str(data)
#Visualizar valores descritivos das variáveis
summary(data)
#Visualizar novamente valores descritivos das variáveis
summary(data)
#converter as variáveis lógicas em 0 e 1, uma vez que algoritmos de aprendizagem automática lidam melhore com valores numéricos
data$cancel<-data$cancel*1
data$rooms<-data$rooms*1
data$breakfast<-data$breakfast*1
#Histograma de todas as variáveis numéricas
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Tratar valores omissos da variável score review através do método da imputação vizinhos mais próximos
set.seed(123)
summary(data)
imputdata1 <- data
imputdata1 <- kNN(data, variable = "score_review", k = 17)
summary(imputdata1)
#K é escolhido através da raiz quadrada do número de observações totais do dataset
summary(imputdata1)
ncol(imputdata1)
head(imputdata1)
#Uma nova coluna de valores lógicos foi adicionada no final do dataset, deve proceder-se à sua remoção
imputdata1 <- subset(imputdata1,select = price:stars)
head(imputdata1)
summary(imputdata1)
data <- imputdata1
#Confirmar a não existência de NA's
apply(data,2,function(x) sum(is.na(x)))
#Visualizar novamente o Histograma de todas as variáveis numéricas após o tratamento dos dados omissos
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Visualização da variável estrelas com ggplot -geometric density
ggplot(data, aes(stars)) + geom_density(adjust = 1/5)
#A análise de densidade da variável sugere a transformação da variável em categórica ordinal
#Alguns alojamentos não possuem classificação por estrelas, de forma a resolver este problema procedeu-se à criação
#de contentores (bins) [0, 1-3, 4-5] de forma a converter a variável estrelas em categórica ordinal
#Criar categorias :
data$stars[data$stars==0]<-'0'
data$stars[data$stars>=1 & data$stars<=3]<-'1-3'
data$stars[data$stars>=4 & data$stars<=5]<-'4-5'
#De forma a melhorar a performance dos algoritmos de aprendizagem automática, de seguida procedeu-se à realização de ordinal encoding
#para a variável estrelas.
#Função para realizar ordinal encoding
encode_ordinal <- function(x, order = unique(x)) {
x <- as.numeric(factor(x, levels = order, exclude = NULL))
x
}
#Aplicar ordinal encoding à variável estrelas
data$stars<- encode_ordinal(data$stars, order = c('0', '1-3', '4-5'))
#Visualizar correlações entre variáveis numéricas através do valor de correlações e do scatter plot
pairs.panels(data[c(1,2,6,7)],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = FALSE,  # show density plots
ellipses = FALSE # show correlation ellipses
)
## Visualização da variável score_review com ggplot -geometric density
ggplot(data, aes(score_review)) + geom_density(adjust = 1/5)
#Verificar os quartis das variáveis numéricas
quantile(data$price)
quantile(data$distance)
quantile(data$ncomments)
#Pela análise dos histogramas parece existir alguns outliers, vamos usar o boxplot para cada variável numérica para confirmar
#which devolve a linha do hotel que constitui um outlier
#Tem que se usar a variável df com os dados originais para identificar os hotéis pertencentes aos outliers
#Verificar quais são os outliers da variável preço
ggplot(data, aes(x="", y=price))+geom_boxplot()
df[which(data$price>quantile(data$price,prob=0.75)+1.5*(quantile(data$price,prob=0.75)-quantile(data$price,prob=0.25))),c(1,2)]
df[which(data$price<quantile(data$price,prob=0.25)-1.5*(quantile(data$price,prob=0.75)-quantile(data$price,prob=0.25))),c(1,2)]
#Verificar quais são os outliers da variável distância
ggplot(data, aes(x="", y=distance))+geom_boxplot()
df[which(data$distance>quantile(data$distance,prob=0.75)+1.5*(quantile(data$distance,prob=0.75)-quantile(data$distance,prob=0.25))),c(1,7)]
df[which(data$distance<quantile(data$distance,prob=0.25)-1.5*(quantile(data$distance,prob=0.75)-quantile(data$distance,prob=0.25))),c(1,7)]
#Verificar quais são os outliers da variável número de comentários
ggplot(data, aes(x="", y=ncomments))+geom_boxplot()
df[which(data$ncomments>quantile(data$ncomments,prob=0.75)+1.5*(quantile(data$ncomments,prob=0.75)-quantile(data$ncomments,prob=0.25))),c(1,3)]
df[which(data$ncomments<quantile(data$ncomments,prob=0.25)-1.5*(quantile(data$ncomments,prob=0.75)-quantile(data$ncomments,prob=0.25))),c(1,3)]
#De seguida procedeu-se à remoção de 1 outlier situado numa posição muito extrema de forma a melhorar a distribuição da variabilidade dos dados
#Remoção da observação com preço igual a 1130
data<-data[-c(which(data$price==1130)), ]
#Através da análise da distribuição e dos outliers foi possível verificar que que algo tem que ser feito para corrigir a distribuição dos dados,
#uma vez que temos uma assimetria dos dados bastante elevada com muitos outliers. Algoritmos de machine learning não lidam bem com distribuições
#assimétricas nem com outliers. Desta forma, vamos de seguida proceder ao logaritmo das observações
#das variaveis distância, número de comentários e preço.
#Verificar novamente as distribuições das variáveis antes de proceder ao logaritmo das variáveis
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#De seguida separa-se um Dataframe sem transformação de logaritmo para caso seja necessário, se possa mais tarde utilizar
data_no_log<-data
#Aplicar os logaritmos às variáveis
data$distance<-log(data$distance)
#Através da análise da distribuição e dos outliers foi possível verificar que que algo tem que ser feito para corrigir a distribuição dos dados,
#uma vez que temos uma assimetria dos dados bastante elevada com muitos outliers. Algoritmos de machine learning não lidam bem com distribuições
#assimétricas nem com outliers. Desta forma, vamos de seguida proceder ao logaritmo das observações
#das variaveis distância, número de comentários e preço.
#Verificar novamente as distribuições das variáveis antes de proceder ao logaritmo das variáveis
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#De seguida separa-se um Dataframe sem transformação de logaritmo para caso seja necessário, se possa mais tarde utilizar
data_no_log<-data
#Aplicar os logaritmos às variáveis
data$distance<-log(data$distance)
data$ncomments<-log1p(data$ncomments)# Soma 1 aos dados visto que a variável tem valores nulos
data$price<-log(data$price)
#Verificar a assimetria da distribuição
skewness(data$distance)
skewness(data$ncomments)
skewness(data$price)
#Verificar a distribuição depois de serem aplicados os logaritmos
data[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
#Visualizar novamente correlações entre variáveis numéricas através do valor de correlações e scatter plot, após a aplicação do logaritmo
pairs.panels(data[c(1,2,6,7)],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = FALSE,  # show density plots
ellipses = FALSE # show correlation ellipses
)
#Verificar novamente outliers após a aplicação do logaritmo
ggplot(data, aes(x="", y=distance))+geom_boxplot()
ggplot(data, aes(x="", y=ncomments))+geom_boxplot()
ggplot(data, aes(x="", y=price))+geom_boxplot()
#De seguida procede-se à separação entre treino e teste
#75% of the sample size
smp_size <- floor(0.75 * nrow(data))
## Definir a semente para obter sempre a mesma separação
set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <- data[train_ind, ]
test <- data[-train_ind, ]
#Normalização dos dados de treino (Média 0 e Desvio Padrão 1)
train_scale<-data_no_log[train_ind, ]
for (i in 1:ncol(train)) train_scale[,i]<-(train[,i]-mean(train[,i]))/sd(train[,i])
#Verificar
round(colMeans(train_scale[,c(1:ncol(train_scale))]),2)
apply(train_scale,2,sd)
#Histograma de dados de treino com estandardização MinMax
train_scale[-c(3,4,5)] %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
<<<<<<< Updated upstream
git
c:\users
c:\Users
=======
>>>>>>> Stashed changes
